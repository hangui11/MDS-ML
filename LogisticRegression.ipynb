{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cd8c78",
   "metadata": {},
   "source": [
    "## Import necessary libraries and load train/test data\n",
    "This cell imports required libraries (pandas, numpy, networkx, ast, torch) and loads the training and test datasets from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d967e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f4955",
   "metadata": {},
   "source": [
    "## Define function to compute centrality features\n",
    "This cell defines a function `centralities` that computes various centrality measures for a given edgelist using NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04061306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    \n",
    "    # Your existing centrality measures\n",
    "    degree = nx.degree_centrality(T)\n",
    "    eigenvector = nx.katz_centrality(T)\n",
    "    closeness = nx.closeness_centrality(T)\n",
    "    # current_flow_closeness = nx.current_flow_closeness_centrality(T)\n",
    "    betweenness = nx.betweenness_centrality(T)\n",
    "    # current_flow_betweenness = nx.current_flow_betweenness_centrality(T)\n",
    "    communicability_betweenness = nx.communicability_betweenness_centrality(T)\n",
    "    # load = nx.load_centrality(T)\n",
    "    subgraph = nx.subgraph_centrality(T)\n",
    "    harmonic = nx.harmonic_centrality(T)\n",
    "    # percolation = nx.percolation_centrality(T)\n",
    "    second_order = nx.second_order_centrality(T)\n",
    "    voterank = nx.voterank(T)\n",
    "    laplacian = nx.laplacian_centrality(T)\n",
    "\n",
    "    features = {}\n",
    "    \n",
    "    for v in T:\n",
    "        features[v] = (\n",
    "            degree[v], \n",
    "            eigenvector[v], \n",
    "            closeness[v],  \n",
    "            betweenness[v], \n",
    "            communicability_betweenness[v],\n",
    "            subgraph[v], \n",
    "            harmonic[v],  \n",
    "            second_order[v],\n",
    "            1 if v in voterank else 0, \n",
    "            laplacian[v],\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de4479",
   "metadata": {},
   "source": [
    "## Split the training data into train and validation sets\n",
    "This cell defines and applies a function to split the training data into training and validation sets based on unique sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fc2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data_set(data, seed=42, test_ratio=0.2):\n",
    "    random.seed(seed)\n",
    "    unique_ids = list(set(data['sentence']))\n",
    "    test_size = int(len(unique_ids) * test_ratio)\n",
    "\n",
    "    test_ids = set(random.sample(unique_ids, test_size))\n",
    "    train_ids = set(unique_ids) - test_ids\n",
    "\n",
    "    train_set = data[data['sentence'].isin(train_ids)]\n",
    "    val_set = data[data['sentence'].isin(test_ids)]\n",
    "\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "train_set, val_set = split_data_set(train, seed=42, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b6343",
   "metadata": {},
   "source": [
    "## Expand the data with centrality features for each vertex\n",
    "This cell defines a function to expand the dataset by computing centrality features for each vertex in each sentence's edgelist, and applies it to train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336f7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expanded_data(data, train=True):\n",
    "    expanded_set = []\n",
    "    for index, row in data.iterrows():\n",
    "        edgelist = ast.literal_eval(row['edgelist'])\n",
    "        central_edges = centralities(edgelist)\n",
    "        language = row['language']\n",
    "        sentence = row['sentence']\n",
    "        n = row['n']\n",
    "        if train:\n",
    "            root = row['root']\n",
    "        else:\n",
    "            id = row['id']\n",
    "        for vertex, values in central_edges.items():\n",
    "            if train:\n",
    "                expanded_set.append((language, sentence, n, vertex, *values, vertex==root))\n",
    "            else:\n",
    "                expanded_set.append((id, language, sentence, n, vertex, *values))\n",
    "\n",
    "    if train:\n",
    "        return pd.DataFrame(expanded_set, columns=['language', 'sentence', 'n', 'vertex', \n",
    "                        'degree', 'eigenvector', 'closeness',\n",
    "                        'betweenness', 'communicability_betweenness',\n",
    "                        'subgraph', 'harmonic',\n",
    "                        'second_order', 'voterank', 'laplacian', \n",
    "                        'is_root'])\n",
    "    \n",
    "    else:\n",
    "        return pd.DataFrame(expanded_set, columns=['id', 'language', 'sentence', 'n', 'vertex', \n",
    "                                                    'degree', 'eigenvector', 'closeness',\n",
    "                                                    'betweenness', 'communicability_betweenness',\n",
    "                                                    'subgraph', 'harmonic',\n",
    "                                                    'second_order', 'voterank', 'laplacian',\n",
    "                                            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e492b",
   "metadata": {},
   "source": [
    "## Sort the expanded dataframes\n",
    "This cell sorts the expanded train, validation, and test dataframes by relevant columns for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4befa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data_train = get_expanded_data(train_set)\n",
    "expanded_data_val = get_expanded_data(val_set)\n",
    "expanded_data_test = get_expanded_data(test, train=False)\n",
    "\n",
    "expanded_data_train.sort_values(by=['language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "expanded_data_val.sort_values(by=['language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "expanded_data_test.sort_values(by=['id', 'language', 'sentence', 'n', 'vertex'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0520283",
   "metadata": {},
   "source": [
    "## Copy expanded dataframes for further processing\n",
    "This cell creates copies of the expanded train, validation, and test dataframes for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d91eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expanded = expanded_data_train.copy()\n",
    "val_expanded = expanded_data_val.copy()\n",
    "test_expanded = expanded_data_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c274d",
   "metadata": {},
   "source": [
    "## Define and apply normalization by sentence\n",
    "This cell defines a function to normalize feature columns within each sentence group using MinMaxScaler, and lists the feature columns to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8700d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "feature_columns = [\n",
    "   'degree', 'eigenvector', 'closeness',\n",
    "    'betweenness', 'communicability_betweenness',\n",
    "    'subgraph', 'harmonic', \n",
    "    'second_order', 'voterank', 'laplacian',\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_by_sentence(df, feature_columns, groupby_cols=['language', 'sentence']):\n",
    "    \"\"\"\n",
    "    Normalize features within each sentence group.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(groupby_cols)\n",
    "    normalized_groups = []\n",
    "\n",
    "    numerical_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    for (lang, sentence), group in grouped:\n",
    "        # Create a StandardScaler for each group\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # Fit and transform the numerical features\n",
    "        normalized = scaler.fit_transform(group[numerical_features])\n",
    "\n",
    "        # Create a DataFrame with the same index and columns as the original group\n",
    "        normalized_df = pd.DataFrame(normalized, index=group.index, columns=numerical_features)\n",
    "\n",
    "        # Combine with non-features columns\n",
    "        combined = group.drop(columns=numerical_features).join(normalized_df)\n",
    "        normalized_groups.append(combined)\n",
    "\n",
    "    normalized_df = pd.concat(normalized_groups)\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a5f59",
   "metadata": {},
   "source": [
    "## Normalize the train, validation, and test sets\n",
    "This cell applies the normalization function to the train, validation, and test expanded dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff3548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = normalize_by_sentence(train_expanded, feature_columns)\n",
    "val_scaled = normalize_by_sentence(val_expanded, feature_columns)\n",
    "test_scaled = normalize_by_sentence(test_expanded, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b6e22",
   "metadata": {},
   "source": [
    "## Train and evaluate a Logistic Regression model\n",
    "This cell prepares the data, applies SMOTEENN for class balancing, trains a Logistic Regression model, and prints a classification report on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a73a863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.70      0.82     38134\n",
      "        True       0.13      0.81      0.22      2100\n",
      "\n",
      "    accuracy                           0.70     40234\n",
      "   macro avg       0.56      0.75      0.52     40234\n",
      "weighted avg       0.94      0.70      0.79     40234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Prepare training data\n",
    "y_train = train_scaled['is_root']\n",
    "X_train = train_scaled.drop(columns=['language', 'sentence', 'n', 'vertex', 'is_root'])\n",
    "\n",
    "# Prepare validation data\n",
    "y_val = val_scaled['is_root']\n",
    "X_val = val_scaled.drop(columns=['language', 'sentence', 'n', 'vertex', 'is_root'])\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTEENN(random_state=42, sampling_strategy=1, n_jobs=-1)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and fit Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "logreg.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = logreg.predict(X_val)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Logistic Regression Classification Report on Validation Set:\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

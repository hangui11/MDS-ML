{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcd4768",
   "metadata": {},
   "source": [
    "# Architecture Model Comparison\n",
    "\n",
    "This notebook compares the performance of Decision Tree, Random Forest, and AdaBoost models, both globally and per language, and explores stacking approaches for similar languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292574fb",
   "metadata": {},
   "source": [
    "## Import libraries and set global configuration\n",
    "\n",
    "Imports all necessary libraries for data processing, graph analysis, machine learning, and model evaluation. Sets the global random seed and defines the feature columns used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d967e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, zero_one_loss, roc_auc_score, make_scorer, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.combine import SMOTEENN\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global configuration\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    'degree', 'closeness', 'betweenness', ' eccentricity',\n",
    "    'leaf_node', 'farness', 'subtree_height'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2b8b6",
   "metadata": {},
   "source": [
    "## Define centrality calculation function\n",
    "\n",
    "Defines a function to compute various centrality measures and subtree heights for each node in a graph, returning a dictionary of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04061306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralities(edgelist):\n",
    "    \n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    degree = nx.degree_centrality(T)  \n",
    "    closeness = nx.closeness_centrality(T)  \n",
    "    betweenness = nx.betweenness_centrality(T)  \n",
    "    eccentricity = nx.eccentricity(T)  \n",
    "    leaf_node = [v for v, d in T.degree() if d == 1]  \n",
    "    Farness = {v: 1 / (eccentricity[v] + 1) for v in T.nodes() if eccentricity[v] > 0}\n",
    "    \n",
    "    # Calculate subtree heights\n",
    "    subtree_height = {}\n",
    "    for node in T.nodes():\n",
    "        subtree_height[node] = nx.single_source_shortest_path_length(T, node)\n",
    "    for node, distances in subtree_height.items():\n",
    "        subtree_height[node] = max(distances.values()) if distances else 0\n",
    "\n",
    "    features = {}\n",
    "    for v in T:\n",
    "        features[v] = (\n",
    "            degree[v],\n",
    "            closeness[v],\n",
    "            betweenness[v], \n",
    "            eccentricity[v],\n",
    "            leaf_node.count(v),\n",
    "            Farness[v],\n",
    "            subtree_height[v]  # Added subtree height feature\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1df441",
   "metadata": {},
   "source": [
    "## Define data splitting function\n",
    "\n",
    "Defines a function to split the dataset into training and validation sets by randomly selecting unique sentences for validation, ensuring no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fc2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data_set(data, seed=SEED, test_ratio=0.2): # Use global SEED\n",
    "    random.seed(seed)\n",
    "    unique_ids = list(set(data['sentence']))\n",
    "    \n",
    "    # Ensure test_size is not larger than the number of unique_ids\n",
    "    if not unique_ids: # Handle empty data\n",
    "        return pd.DataFrame(columns=data.columns), pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    actual_test_size = int(len(unique_ids) * test_ratio)\n",
    "    if actual_test_size == 0 and len(unique_ids) > 0 and test_ratio > 0: # Ensure at least one sample if possible\n",
    "        actual_test_size = 1 \n",
    "    if actual_test_size > len(unique_ids): # Cap test size\n",
    "        actual_test_size = len(unique_ids)\n",
    "\n",
    "    if actual_test_size == 0 : # if still zero (e.g. unique_ids is empty or test_ratio is 0)\n",
    "         test_ids = set()\n",
    "    else:\n",
    "        test_ids = set(random.sample(unique_ids, actual_test_size))\n",
    "\n",
    "    train_ids = set(unique_ids) - test_ids\n",
    "\n",
    "    train_set = data[data['sentence'].isin(train_ids)]\n",
    "    val_set = data[data['sentence'].isin(test_ids)]\n",
    "\n",
    "    return train_set, val_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30962df6",
   "metadata": {},
   "source": [
    "## Utility functions for feature extraction, normalization, and cross-validation\n",
    "\n",
    "Defines utility functions for centrality calculation, data splitting, feature expansion, normalization, and k-fold cross-validation index creation. Also includes the main data preparation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336f7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \"\"\"Calculate centrality measures for graph nodes.\"\"\"\n",
    "    if not edgelist:\n",
    "        return {}\n",
    "    \n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    \n",
    "    degree = nx.degree_centrality(T)\n",
    "    closeness = nx.closeness_centrality(T)\n",
    "    betweenness = nx.betweenness_centrality(T)\n",
    "    eccentricity = nx.eccentricity(T)\n",
    "    leaf_node = [v for v, d in T.degree() if d == 1]\n",
    "    farness = {v: 1 / (eccentricity[v] + 1) for v in T.nodes() if eccentricity[v] > 0}\n",
    "    \n",
    "    # Calculate subtree heights\n",
    "    subtree_height = {}\n",
    "    for node in T.nodes():\n",
    "        distances = nx.single_source_shortest_path_length(T, node)\n",
    "        subtree_height[node] = max(distances.values()) if distances else 0\n",
    "\n",
    "    features = {}\n",
    "    for v in T:\n",
    "        features[v] = (\n",
    "            degree[v], closeness[v], betweenness[v], eccentricity[v],\n",
    "            leaf_node.count(v), farness[v], subtree_height[v]\n",
    "        )\n",
    "    return features\n",
    "\n",
    "def split_data_set(data, seed=SEED, test_ratio=0.2):\n",
    "    \"\"\"Split data by unique sentences.\"\"\"\n",
    "    random.seed(seed)\n",
    "    unique_ids = list(set(data['sentence']))\n",
    "    \n",
    "    if not unique_ids:\n",
    "        return pd.DataFrame(columns=data.columns), pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    test_size = max(1, int(len(unique_ids) * test_ratio))\n",
    "    test_size = min(test_size, len(unique_ids))\n",
    "    \n",
    "    if test_size == 0:\n",
    "        test_ids = set()\n",
    "    else:\n",
    "        test_ids = set(random.sample(unique_ids, test_size))\n",
    "    \n",
    "    train_ids = set(unique_ids) - test_ids\n",
    "    \n",
    "    train_set = data[data['sentence'].isin(train_ids)]\n",
    "    val_set = data[data['sentence'].isin(test_ids)]\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def get_expanded_data(data, train=True):\n",
    "    \"\"\"Expand data with centrality features.\"\"\"\n",
    "    expanded_set = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        edgelist_str = row.get('rand_edgelist', '[]')\n",
    "        try:\n",
    "            edgelist = ast.literal_eval(edgelist_str)\n",
    "            if not isinstance(edgelist, list):\n",
    "                edgelist = []\n",
    "        except (ValueError, SyntaxError):\n",
    "            edgelist = []\n",
    "\n",
    "        central_edges = centralities(edgelist)\n",
    "        language = row['language']\n",
    "        sentence = row['sentence']\n",
    "        n = row['n']\n",
    "        \n",
    "        if train:\n",
    "            root = row['root']\n",
    "            for vertex, values in central_edges.items():\n",
    "                expanded_set.append((language, sentence, n, vertex, *values, vertex==root))\n",
    "        else:\n",
    "            row_id = row.get('id', None)\n",
    "            for vertex, values in central_edges.items():\n",
    "                expanded_set.append((row_id, language, sentence, n, vertex, *values))\n",
    "\n",
    "    if train:\n",
    "        columns = ['language', 'sentence', 'n', 'vertex'] + FEATURE_COLUMNS + ['is_root']\n",
    "    else:\n",
    "        columns = ['id', 'language', 'sentence', 'n', 'vertex'] + FEATURE_COLUMNS\n",
    "    \n",
    "    return pd.DataFrame(expanded_set, columns=columns)\n",
    "\n",
    "def normalize_by_sentence(df, feature_columns, groupby_cols=['language', 'sentence']):\n",
    "    \"\"\"Normalize features within each sentence group.\"\"\"\n",
    "    grouped = df.groupby(groupby_cols)\n",
    "    normalized_groups = []\n",
    "    numerical_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    for (lang, sentence), group in grouped:\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized = scaler.fit_transform(group[numerical_features])\n",
    "        normalized_df = pd.DataFrame(normalized, index=group.index, columns=numerical_features)\n",
    "        combined = group.drop(columns=numerical_features).join(normalized_df)\n",
    "        normalized_groups.append(combined)\n",
    "\n",
    "    return pd.concat(normalized_groups)\n",
    "\n",
    "def create_k_folds_indices(data, k=5, seed_base=SEED):\n",
    "    \"\"\"Create k-fold indices for cross-validation.\"\"\"\n",
    "    splits = []\n",
    "    \n",
    "    if not data.index.is_unique:\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "    for i in range(k):\n",
    "        current_seed = seed_base + i\n",
    "        fold_train_set, fold_val_set = split_data_set(data, seed=current_seed, test_ratio=0.2)\n",
    "        \n",
    "        train_indices = data.index.get_indexer(fold_train_set.index.values)\n",
    "        val_indices = data.index.get_indexer(fold_val_set.index.values)\n",
    "        \n",
    "        splits.append((train_indices, val_indices))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare training and test data.\"\"\"\n",
    "    train = pd.read_csv('train-random.csv')\n",
    "    test = pd.read_csv('test-random.csv')\n",
    "    \n",
    "    # Split training data\n",
    "    train_set, val_set = split_data_set(train, seed=SEED, test_ratio=0.2)\n",
    "    \n",
    "    # Expand data with features\n",
    "    expanded_data_train = get_expanded_data(train_set)\n",
    "    expanded_data_val = get_expanded_data(val_set)\n",
    "    expanded_data_test = get_expanded_data(test, train=False)\n",
    "    \n",
    "    # Sort data\n",
    "    expanded_data_train.sort_values(by=['language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "    expanded_data_val.sort_values(by=['language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "    expanded_data_test.sort_values(by=['id', 'language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "    \n",
    "    # Normalize features\n",
    "    train_scaled = normalize_by_sentence(expanded_data_train, FEATURE_COLUMNS)\n",
    "    val_scaled = normalize_by_sentence(expanded_data_val, FEATURE_COLUMNS)\n",
    "    test_scaled = normalize_by_sentence(expanded_data_test, FEATURE_COLUMNS)\n",
    "    \n",
    "    return train_scaled, val_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00f1a7",
   "metadata": {},
   "source": [
    "## Prepare the data for training and validation\n",
    "\n",
    "Calls the data preparation function to load, process, and normalize the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4623e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, val_scaled, test_scaled = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabd8a0",
   "metadata": {},
   "source": [
    "## Model training functions\n",
    "\n",
    "Defines functions to train Decision Tree, Random Forest, and AdaBoost models, both globally and per language. Each function handles resampling, training, validation, and model saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a61c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "def train_individual_dt_models(train_scaled, val_scaled):\n",
    "    \"\"\"Train Decision Tree models: one for all data, and one for each language.\"\"\"\n",
    "    grouped_train = train_scaled.groupby(['language'])\n",
    "    grouped_val = val_scaled.groupby(['language']) \n",
    "    models = {}\n",
    "\n",
    "    # Ensure directory exists\n",
    "    dt_dir = \"./dt_models\"\n",
    "    os.makedirs(dt_dir, exist_ok=True)\n",
    "\n",
    "    # Train on all data first\n",
    "    X_all = train_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "    y_all = train_scaled['is_root'].astype(int)\n",
    "    smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "    X_all_resampled, y_all_resampled = smote_enn.fit_resample(X_all, y_all)\n",
    "    model_all = DecisionTreeClassifier(class_weight='balanced', random_state=SEED)\n",
    "    model_all.fit(X_all_resampled, y_all_resampled)\n",
    "    #models['all_languages'] = model_all\n",
    "\n",
    "    # Validation on all data\n",
    "    X_val_all = val_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "    y_val_all = val_scaled['is_root']\n",
    "    \n",
    "    y_pred_all = model_all.predict(X_val_all)\n",
    "    accuracy_all = classification_report(y_val_all, y_pred_all)\n",
    "    print(f\"Decision Tree Results for ALL languages:\")\n",
    "    print(accuracy_all)\n",
    "    filename_all = os.path.join(dt_dir, \"dt_model_all_languages.joblib\")\n",
    "    joblib.dump(model_all, filename_all)\n",
    "\n",
    "    # Train per-language models\n",
    "    for lang, group in grouped_train:\n",
    "        # If lang is a tuple (e.g., ('Arabic',)), extract the string\n",
    "        if isinstance(lang, tuple):\n",
    "            lang_str = lang[0]\n",
    "        else:\n",
    "            lang_str = lang\n",
    "\n",
    "        group_data = group.drop(columns=['language', 'n', 'vertex'])\n",
    "        \n",
    "        X = group_data.drop(columns=['sentence', 'is_root'])\n",
    "        y = group_data['is_root'].astype(int)\n",
    "        \n",
    "        # Apply SMOTEENN resampling\n",
    "        smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "        \n",
    "        # Train model\n",
    "        model = DecisionTreeClassifier(class_weight='balanced', random_state=SEED)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        models[lang_str] = model\n",
    "        \n",
    "        X_val_group = grouped_val.get_group(lang)\n",
    "        y_val_group = X_val_group['is_root']\n",
    "        X_val_group = X_val_group.drop(columns=['language', 'sentence', 'n', 'vertex', 'is_root'])\n",
    "\n",
    "        y_pred = model.predict(X_val_group)\n",
    "        accuracy = classification_report(y_val_group, y_pred)\n",
    "        #print(f\"Decision Tree Results for {lang_str}:\")\n",
    "        #print(accuracy)\n",
    "        \n",
    "        # Save model\n",
    "        filename = os.path.join(dt_dir, f\"dt_model_{lang_str}_.joblib\")\n",
    "        joblib.dump(model, filename)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def train_individual_rf_models(train_scaled, val_scaled):\n",
    "    \"\"\"Train Random Forest models: one for all data, and one for each language.\"\"\"\n",
    "    grouped_train = train_scaled.groupby(['language'])\n",
    "    grouped_val = val_scaled.groupby(['language'])\n",
    "    models = {}\n",
    "\n",
    "    # Ensure directory exists\n",
    "    rf_dir = \"./rf_models\"\n",
    "    os.makedirs(rf_dir, exist_ok=True)\n",
    "\n",
    "    # Train on all data first\n",
    "    X_all = train_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "    y_all = train_scaled['is_root'].astype(int)\n",
    "    smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "    X_all_resampled, y_all_resampled = smote_enn.fit_resample(X_all, y_all)\n",
    "    model_all = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight='balanced',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_all.fit(X_all_resampled, y_all_resampled)\n",
    "    #models['all_languages'] = model_all\n",
    "\n",
    "    # Validation on all data\n",
    "    X_val_all = val_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "    y_val_all = val_scaled['is_root']\n",
    "    y_pred_all = model_all.predict(X_val_all)\n",
    "    accuracy_all = classification_report(y_val_all, y_pred_all)\n",
    "    print(f\"Random Forest Results for ALL languages:\")\n",
    "    print(accuracy_all)\n",
    "    filename_all = os.path.join(rf_dir, \"rf_model_all_languages.joblib\")\n",
    "    joblib.dump(model_all, filename_all)\n",
    "\n",
    "    # Train per-language models\n",
    "    for lang, group in grouped_train:\n",
    "        if isinstance(lang, tuple):\n",
    "            lang_str = lang[0]\n",
    "        else:\n",
    "            lang_str = lang\n",
    "\n",
    "        group_data = group.drop(columns=['language', 'n', 'vertex'])\n",
    "        \n",
    "        X = group_data.drop(columns=['sentence', 'is_root'])\n",
    "        y = group_data['is_root'].astype(int)\n",
    "        \n",
    "        # Apply SMOTEENN resampling\n",
    "        smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight='balanced',\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        models[lang_str] = model\n",
    "        \n",
    "        X_val_group = grouped_val.get_group(lang)\n",
    "        y_val_group = X_val_group['is_root']\n",
    "        X_val_group = X_val_group.drop(columns=['language', 'sentence', 'n', 'vertex', 'is_root'])\n",
    "\n",
    "        y_pred = model.predict(X_val_group)\n",
    "        accuracy = classification_report(y_val_group, y_pred)\n",
    "        #print(f\"Random Forest Results for {lang_str}:\")\n",
    "        #print(accuracy)\n",
    "        \n",
    "        # Save model\n",
    "        filename = os.path.join(rf_dir, f\"rf_model_{lang_str}_.joblib\")\n",
    "        joblib.dump(model, filename)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def train_individual_ada_models(train_scaled, val_scaled):\n",
    "    \"\"\"Train AdaBoost models: one for all data, and one for each language.\"\"\"\n",
    "    grouped_train = train_scaled.groupby(['language'])\n",
    "    grouped_val = val_scaled.groupby(['language'])\n",
    "    models = {}\n",
    "\n",
    "    # Ensure directory exists\n",
    "    ada_dir = \"./ada_models\"\n",
    "    os.makedirs(ada_dir, exist_ok=True)\n",
    "\n",
    "    # Train on all data first\n",
    "    X_all = train_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "    y_all = train_scaled['is_root'].astype(int)\n",
    "    smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "    X_all_resampled, y_all_resampled = smote_enn.fit_resample(X_all, y_all)\n",
    "    base_dt = DecisionTreeClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model_all = AdaBoostClassifier(\n",
    "        estimator=base_dt,\n",
    "        n_estimators=100,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model_all.fit(X_all_resampled, y_all_resampled)\n",
    "    #models['all_languages'] = model_all\n",
    "\n",
    "    # Validation on all data\n",
    "    X_val_all = val_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "    y_val_all = val_scaled['is_root']\n",
    "    y_pred_all = model_all.predict(X_val_all)\n",
    "    accuracy_all = classification_report(y_val_all, y_pred_all)\n",
    "    print(f\"AdaBoost Results for ALL languages:\")\n",
    "    print(accuracy_all)\n",
    "    filename_all = os.path.join(ada_dir, \"ada_model_all_languages.joblib\")\n",
    "    joblib.dump(model_all, filename_all)\n",
    "\n",
    "    # Train per-language models\n",
    "    for lang, group in grouped_train:\n",
    "        if isinstance(lang, tuple):\n",
    "            lang_str = lang[0]\n",
    "        else:\n",
    "            lang_str = lang\n",
    "\n",
    "        group_data = group.drop(columns=['language', 'n', 'vertex'])\n",
    "        \n",
    "        X = group_data.drop(columns=['sentence', 'is_root'])\n",
    "        y = group_data['is_root'].astype(int)\n",
    "        \n",
    "        # Apply SMOTEENN resampling\n",
    "        smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "        \n",
    "        # Train model\n",
    "        base_dt = DecisionTreeClassifier(\n",
    "            class_weight='balanced',\n",
    "            random_state=SEED\n",
    "        )\n",
    "        model = AdaBoostClassifier(\n",
    "            estimator=base_dt,\n",
    "            n_estimators=50,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        models[lang_str] = model\n",
    "        \n",
    "        X_val_group = grouped_val.get_group(lang)\n",
    "        y_val_group = X_val_group['is_root']\n",
    "        X_val_group = X_val_group.drop(columns=['language', 'sentence', 'n', 'vertex', 'is_root'])\n",
    "\n",
    "        y_pred = model.predict(X_val_group)\n",
    "        accuracy = classification_report(y_val_group, y_pred)\n",
    "        #print(f\"AdaBoost Results for {lang_str}:\")\n",
    "        #print(accuracy)\n",
    "        \n",
    "        # Save model\n",
    "        filename = os.path.join(ada_dir, f\"ada_model_{lang_str}_.joblib\")\n",
    "        joblib.dump(model, filename)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995938c",
   "metadata": {},
   "source": [
    "## Train all models\n",
    "\n",
    "Trains Decision Tree, Random Forest, and AdaBoost models on the training data and saves the resulting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af94c5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results for ALL languages:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.88      0.92     38134\n",
      "        True       0.16      0.42      0.24      2100\n",
      "\n",
      "    accuracy                           0.86     40234\n",
      "   macro avg       0.56      0.65      0.58     40234\n",
      "weighted avg       0.92      0.86      0.89     40234\n",
      "\n",
      "Random Forest Results for ALL languages:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.91      0.93     38134\n",
      "        True       0.19      0.40      0.26      2100\n",
      "\n",
      "    accuracy                           0.88     40234\n",
      "   macro avg       0.58      0.65      0.60     40234\n",
      "weighted avg       0.92      0.88      0.90     40234\n",
      "\n",
      "AdaBoost Results for ALL languages:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.89      0.92     38134\n",
      "        True       0.17      0.41      0.24      2100\n",
      "\n",
      "    accuracy                           0.86     40234\n",
      "   macro avg       0.57      0.65      0.58     40234\n",
      "weighted avg       0.92      0.86      0.89     40234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtmodels = train_individual_dt_models(train_scaled,val_scaled)\n",
    "rfmodels = train_individual_rf_models(train_scaled,val_scaled)\n",
    "ada_models = train_individual_ada_models(train_scaled,val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93fe3f",
   "metadata": {},
   "source": [
    "## Calculate pairwise model similarities\n",
    "\n",
    "Defines a function to compute pairwise similarities between models trained on different languages, based on prediction agreement on validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5e2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_model_similarities(loaded_models, val_scaled, similarity_threshold=0.3):\n",
    "    \"\"\"Calculate pairwise model similarities.\"\"\"\n",
    "    pairwise_similarity = {}\n",
    "    languages_val = sorted(set(val_scaled['language']))\n",
    "    \n",
    "    # Prepare validation data\n",
    "    val_data_X = {}\n",
    "    val_data_y = {}\n",
    "    grouped_val = val_scaled.groupby(['language'])\n",
    "    \n",
    "    for lang in languages_val:\n",
    "        if lang in grouped_val.groups:\n",
    "            val_group_df = grouped_val.get_group(lang)\n",
    "            X_val = val_group_df[FEATURE_COLUMNS].copy()\n",
    "            y_val = val_group_df['is_root'].astype(int).copy()\n",
    "            val_data_X[lang] = X_val\n",
    "            val_data_y[lang] = y_val\n",
    "    \n",
    "    model_names = list(loaded_models.keys())\n",
    "    #print(model_names)\n",
    "    \n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i + 1, len(model_names)):\n",
    "            lang_i = languages_val[i]\n",
    "            lang_j = languages_val[j]\n",
    "            \n",
    "            model_i = loaded_models[model_names[i]]\n",
    "            model_j = loaded_models[model_names[j]]\n",
    "            \n",
    "            X_val_i = val_data_X[lang_i]\n",
    "            X_val_j = val_data_X[lang_j]\n",
    "            \n",
    "            # Calculate similarities\n",
    "            preds_i_on_i = model_i.predict(X_val_i)\n",
    "            preds_j_on_i = model_j.predict(X_val_i)\n",
    "            preds_i_on_j = model_i.predict(X_val_j)\n",
    "            preds_j_on_j = model_j.predict(X_val_j)\n",
    "            \n",
    "            similarity_on_i = accuracy_score(preds_i_on_i, preds_j_on_i)\n",
    "            similarity_on_j = accuracy_score(preds_i_on_j, preds_j_on_j)\n",
    "            \n",
    "            if similarity_on_i >= similarity_threshold and similarity_on_j >= similarity_threshold:\n",
    "                if lang_i not in pairwise_similarity:\n",
    "                    pairwise_similarity[lang_i] = []\n",
    "                if lang_j not in pairwise_similarity:\n",
    "                    pairwise_similarity[lang_j] = []\n",
    "                \n",
    "                if lang_j not in pairwise_similarity[lang_i]:\n",
    "                    pairwise_similarity[lang_i].append(lang_j)\n",
    "                if lang_i not in pairwise_similarity[lang_j]:\n",
    "                    pairwise_similarity[lang_j].append(lang_i)\n",
    "    \n",
    "    return pairwise_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a8a9c",
   "metadata": {},
   "source": [
    "## Compute pairwise similarities for all model types\n",
    "\n",
    "Calculates pairwise similarities for Decision Tree, Random Forest, and AdaBoost models using a specified similarity threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32aca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_similarity_dt = calculate_model_similarities(dtmodels, val_scaled, similarity_threshold=0.85)\n",
    "pairwise_similarity_rf = calculate_model_similarities(rfmodels, val_scaled, similarity_threshold=0.85)\n",
    "pairwise_similarity_ada = calculate_model_similarities(ada_models, val_scaled, similarity_threshold=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251b993",
   "metadata": {},
   "source": [
    "## Find language groups based on model similarity\n",
    "\n",
    "Defines a function to find groups of languages whose models are mutually similar, using a clique-finding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921dfd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_language_groups(lang_dict):\n",
    "    \"\"\"\n",
    "    Find groups of languages where each language appears in all other group members' lists.\n",
    "    Each language can only be grouped once.\n",
    "    \"\"\"\n",
    "    \n",
    "    def can_form_group(languages):\n",
    "        \"\"\"Check if a set of languages can form a valid group\"\"\"\n",
    "        for lang in languages:\n",
    "            if lang not in lang_dict:\n",
    "                return False\n",
    "            # Check if all other languages in the group are in this language's list\n",
    "            others = languages - {lang}\n",
    "            if not others.issubset(set(lang_dict[lang])):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def find_all_cliques(available_langs):\n",
    "        \"\"\"Find all maximal cliques from available languages\"\"\"\n",
    "        cliques = []\n",
    "        \n",
    "        # Start with each language as a potential clique\n",
    "        for start_lang in available_langs:\n",
    "            if start_lang not in lang_dict:\n",
    "                continue\n",
    "                \n",
    "            # Build clique starting from this language\n",
    "            current_clique = {start_lang}\n",
    "            candidates = set(lang_dict[start_lang]) & available_langs\n",
    "            \n",
    "            # Greedily add languages that are connected to all in current clique\n",
    "            for candidate in sorted(candidates):  # Sort for consistent results\n",
    "                if candidate in available_langs and can_form_group(current_clique | {candidate}):\n",
    "                    current_clique.add(candidate)\n",
    "            \n",
    "            # Only keep if it's a valid clique of size > 1\n",
    "            if len(current_clique) > 1 and can_form_group(current_clique):\n",
    "                cliques.append(current_clique)\n",
    "        \n",
    "        return cliques\n",
    "    \n",
    "    # Find groups iteratively\n",
    "    groups = []\n",
    "    used_languages = set()\n",
    "    available_languages = set(lang_dict.keys())\n",
    "    \n",
    "    while available_languages:\n",
    "        # Find all possible cliques from remaining languages\n",
    "        cliques = find_all_cliques(available_languages)\n",
    "        \n",
    "        if not cliques:\n",
    "            break\n",
    "            \n",
    "        # Choose the largest clique (greedy approach)\n",
    "        best_clique = max(cliques, key=len)\n",
    "        groups.append(sorted(list(best_clique)))\n",
    "        \n",
    "        # Remove used languages\n",
    "        used_languages.update(best_clique)\n",
    "        available_languages -= best_clique\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9423de",
   "metadata": {},
   "source": [
    "## Group languages for stacking\n",
    "\n",
    "Finds and stores groups of similar languages for each model type, to be used in stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9988db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dt = find_language_groups(pairwise_similarity_dt)\n",
    "group_rf = find_language_groups(pairwise_similarity_rf)\n",
    "group_ada = find_language_groups(pairwise_similarity_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ee43f",
   "metadata": {},
   "source": [
    "## Create stacked models for language groups\n",
    "\n",
    "Defines a function to create stacked models for groups of similar languages, using a meta-classifier (Decision Tree, Random Forest, or AdaBoost) as the final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0b2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_models(loaded_models, train_scaled, pred_group, meta_type='rf'):\n",
    "    \"\"\"\n",
    "    Create stacked models for similar language groups.\n",
    "\n",
    "    Parameters:\n",
    "    - loaded_models: dict of base models\n",
    "    - train_scaled: DataFrame with training data\n",
    "    - meta_type: str, one of {'rf', 'dt', 'ada'} to select meta-classifier\n",
    "        'rf'  = RandomForestClassifier (default)\n",
    "        'dt'  = DecisionTreeClassifier\n",
    "        'ada' = AdaBoostClassifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define predefined groups\n",
    "    predefined_groups_lang_names = pred_group\n",
    "    \n",
    "    model_groups_for_stacking = []\n",
    "    \n",
    "    # Convert language names to tuple format\n",
    "    for name_group in predefined_groups_lang_names:\n",
    "        tuple_group = []\n",
    "        for lang_name in name_group:\n",
    "            if lang_name in loaded_models:\n",
    "                tuple_group.append(lang_name)\n",
    "        if tuple_group:\n",
    "            model_groups_for_stacking.append(sorted(tuple_group))\n",
    "\n",
    "    \n",
    "    # Add remaining languages as individual groups\n",
    "    all_languages = set(loaded_models.keys())\n",
    "    languages_in_groups = set()\n",
    "\n",
    "    for group in model_groups_for_stacking:\n",
    "        for lang_tuple in group:\n",
    "            languages_in_groups.add(lang_tuple)\n",
    "    \n",
    "    for lang_tuple in all_languages:\n",
    "        if lang_tuple not in languages_in_groups:\n",
    "            model_groups_for_stacking.append([lang_tuple])\n",
    "    \n",
    "    # Prepare resampled training data\n",
    "    train_X_resampled_map = {}\n",
    "    train_y_resampled_map = {}\n",
    "    grouped_train = train_scaled.groupby(['language'])\n",
    "    smote_enn = SMOTEENN(random_state=SEED, sampling_strategy=0.5, n_jobs=-1)\n",
    "    \n",
    "    for lang_tuple, group_df in grouped_train:\n",
    "        if isinstance(lang_tuple, tuple):\n",
    "            lang_str = lang_tuple[0]\n",
    "        X_res = group_df[FEATURE_COLUMNS].copy()\n",
    "        y_res = group_df['is_root'].astype(int).copy()\n",
    "        \n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X_res, y_res)\n",
    "        \n",
    "        train_X_resampled_map[lang_str] = pd.DataFrame(X_resampled, columns=FEATURE_COLUMNS)\n",
    "        train_y_resampled_map[lang_str] = pd.Series(y_resampled, name='is_root')\n",
    "    \n",
    "    # Select meta-estimator based on meta_type\n",
    "    if meta_type == 'rf':\n",
    "        meta_estimator = RandomForestClassifier(\n",
    "            n_estimators=100, random_state=SEED, class_weight='balanced', n_jobs=-1\n",
    "        )\n",
    "    elif meta_type == 'dt':\n",
    "        meta_estimator = DecisionTreeClassifier(\n",
    "            max_depth=10, random_state=SEED, class_weight='balanced'\n",
    "        )\n",
    "    elif meta_type == 'ada':\n",
    "        meta_estimator = AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=5, random_state=SEED, class_weight='balanced'),\n",
    "            n_estimators=50, random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"meta_type must be one of {'rf', 'dt', 'ada'}\")\n",
    "    \n",
    "    # Create stacked models\n",
    "    stacked_models_final = {}\n",
    "    print (model_groups_for_stacking)\n",
    "    for i, lang_group in enumerate(model_groups_for_stacking):\n",
    "        group_name = f\"stacked_group_{i+1}\"\n",
    "        \n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        base_estimators = []\n",
    "        \n",
    "        for lang_tuple in lang_group:\n",
    "            \n",
    "            X_train_list.append(train_X_resampled_map[lang_tuple])\n",
    "            y_train_list.append(train_y_resampled_map[lang_tuple])\n",
    "        \n",
    "            if isinstance(lang_tuple, tuple):\n",
    "                estimator_name = lang_tuple[0]\n",
    "            else:\n",
    "                estimator_name = lang_tuple\n",
    "            base_estimators.append((estimator_name, loaded_models[lang_tuple]))\n",
    "      \n",
    "        X_train_combined = pd.concat(X_train_list, ignore_index=True)\n",
    "        y_train_combined = pd.concat(y_train_list, ignore_index=True)\n",
    "        \n",
    "        if len(base_estimators) == 1:\n",
    "            # Use original model for single-model groups\n",
    "            stacked_models_final[estimator_name] = base_estimators[0][1]\n",
    "        else:\n",
    "            # Create stacking classifier\n",
    "            #group_splits = create_k_folds_indices(X_train_combined)\n",
    "            \n",
    "            stacking_clf = StackingClassifier(\n",
    "                estimators=base_estimators,\n",
    "                final_estimator=meta_estimator,\n",
    "                stack_method='auto',\n",
    "                n_jobs=-1,\n",
    "                passthrough=False\n",
    "            )\n",
    "            \n",
    "            stacking_clf.fit(X_train_combined, y_train_combined)\n",
    "            stacked_models_final[group_name] = stacking_clf\n",
    "            \n",
    "            # Save model\n",
    "            if meta_type == 'dt':\n",
    "                filename = f\"./dt_models/{group_name}_stacked_dt.joblib\"\n",
    "            elif meta_type == 'rf':\n",
    "                filename = f\"./rf_models/{group_name}_stacked_rf.joblib\"\n",
    "            elif meta_type == 'ada':\n",
    "                filename = f\"./ada_models/{group_name}_stacked_ada.joblib\"\n",
    "            joblib.dump(stacking_clf, filename)\n",
    "    \n",
    "    # Create language to model mapping\n",
    "    # language_to_model_map = {}\n",
    "    # for i, lang_group in enumerate(model_groups_for_stacking):\n",
    "    #     group_key = f\"stacked_group_{i+1}\"\n",
    "    #     if group_key in stacked_models_final:\n",
    "    #         model = stacked_models_final[group_key]\n",
    "    #         for lang_tuple in lang_group:\n",
    "    #             print(lang_tuple)\n",
    "    #             language_to_model_map[lang_tuple] = model\n",
    "    \n",
    "    return stacked_models_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb1c39",
   "metadata": {},
   "source": [
    "## Train stacked models for each model type\n",
    "\n",
    "Creates stacked models for each group of similar languages, for Decision Tree, Random Forest, and AdaBoost base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d981c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Galician', 'Icelandic', 'Spanish', 'Swedish'], ['German', 'Italian', 'Turkish'], ['Arabic', 'Finnish', 'Polish'], ['Indonesian', 'Portuguese', 'Russian'], ['Thai'], ['Japanese'], ['English'], ['French'], ['Korean'], ['Chinese'], ['Czech'], ['Hindi']]\n",
      "[['Arabic', 'Chinese', 'Czech', 'Finnish', 'French', 'Galician', 'Hindi', 'Indonesian', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Spanish', 'Swedish', 'Thai'], ['English', 'German', 'Italian', 'Japanese'], ['Icelandic'], ['Turkish']]\n",
      "[['Czech', 'Galician', 'Indonesian', 'Polish', 'Portuguese', 'Swedish'], ['German', 'Italian', 'Spanish', 'Turkish'], ['Arabic', 'Icelandic', 'Russian'], ['English', 'Finnish'], ['Thai'], ['Japanese'], ['French'], ['Korean'], ['Chinese'], ['Hindi']]\n"
     ]
    }
   ],
   "source": [
    "language_to_model_map_dt = create_stacked_models(dtmodels, train_scaled, group_dt, meta_type='dt')\n",
    "language_to_model_map_rf = create_stacked_models(rfmodels, train_scaled, group_rf, meta_type='rf')\n",
    "language_to_model_map_ada = create_stacked_models(ada_models, train_scaled, group_ada, meta_type='ada')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3e895",
   "metadata": {},
   "source": [
    "## Compute classification reports for stacked models\n",
    "\n",
    "Defines a function to compute and print classification reports for the stacked models on the validation set, aggregating predictions across all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069a3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_separate_datasets(stacked_models, val_scaled):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import classification_report as skl_classification_report\n",
    "\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    grouped_val = val_scaled.groupby(['language'])\n",
    "\n",
    "    # Corrected version: properly matches languages to models and handles groupings\n",
    "    for group_name, model in stacked_models.items():\n",
    "        # Determine which languages this model is responsible for\n",
    "        if isinstance(model, StackingClassifier):\n",
    "            # For stacking, group_name is like 'stacked_group_X', so need to infer languages\n",
    "            # Assume language names are in the base estimators\n",
    "            language_names = [name for name, _ in model.estimators]\n",
    "        else:\n",
    "            language_names = [group_name]\n",
    "\n",
    "        for lang in language_names:\n",
    "            # grouped_val.groups keys are language names, not tuples\n",
    "            if lang in grouped_val.groups:\n",
    "                X_val_group = grouped_val.get_group(lang)[FEATURE_COLUMNS]\n",
    "                y_val_group = grouped_val.get_group(lang)['is_root']\n",
    "\n",
    "                # Make predictions for this group\n",
    "                y_pred_group = model.predict(X_val_group)\n",
    "\n",
    "                # Collect predictions and true labels\n",
    "                all_predictions.extend(y_pred_group)\n",
    "                all_true_labels.extend(y_val_group)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    # Calculate metrics\n",
    "    # Avoid shadowing the imported classification_report\n",
    "    classification_rep = skl_classification_report(all_true_labels, all_predictions)\n",
    "\n",
    "    return classification_rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eff7fd",
   "metadata": {},
   "source": [
    "## Print classification reports for all model types\n",
    "\n",
    "Computes and prints the classification reports for Decision Tree, Random Forest, and AdaBoost stacked models on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b475fe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.89      0.92     38134\n",
      "        True       0.16      0.39      0.22      2100\n",
      "\n",
      "    accuracy                           0.86     40234\n",
      "   macro avg       0.56      0.64      0.57     40234\n",
      "weighted avg       0.92      0.86      0.89     40234\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.86      0.91     38134\n",
      "        True       0.16      0.48      0.24      2100\n",
      "\n",
      "    accuracy                           0.84     40234\n",
      "   macro avg       0.56      0.67      0.57     40234\n",
      "weighted avg       0.93      0.84      0.87     40234\n",
      "\n",
      "AdaBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.84      0.90     38134\n",
      "        True       0.15      0.50      0.23      2100\n",
      "\n",
      "    accuracy                           0.83     40234\n",
      "   macro avg       0.56      0.67      0.57     40234\n",
      "weighted avg       0.93      0.83      0.87     40234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_dt = compute_accuracy_separate_datasets(language_to_model_map_dt, val_scaled)\n",
    "classification_report_rf = compute_accuracy_separate_datasets(language_to_model_map_rf, val_scaled)\n",
    "classification_report_ada = compute_accuracy_separate_datasets(language_to_model_map_ada, val_scaled)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report_dt)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report_rf)\n",
    "print(\"AdaBoost Classification Report:\")\n",
    "print(classification_report_ada)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

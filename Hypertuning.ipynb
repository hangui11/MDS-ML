{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b48e84",
   "metadata": {},
   "source": [
    "# Hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18936b68",
   "metadata": {},
   "source": [
    "## Import necessary libraries and set up global configuration\n",
    "\n",
    "This cell imports all required libraries for data processing, graph analysis, machine learning, and model evaluation. It also sets a global random seed and defines the feature columns used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda35483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, zero_one_loss, roc_auc_score, make_scorer, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.combine import SMOTEENN\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global configuration\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    'degree', 'closeness', 'betweenness', ' eccentricity',\n",
    "    'leaf_node', 'farness', 'subtree_height'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf73d8",
   "metadata": {},
   "source": [
    "## Define function to compute centrality features\n",
    "\n",
    "This cell defines the `centralities` function, which computes various graph centrality measures for each node in a given edgelist using NetworkX. The function returns a dictionary of features for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92903595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralities(edgelist):\n",
    "    \n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    degree = nx.degree_centrality(T)  \n",
    "    closeness = nx.closeness_centrality(T)  \n",
    "    betweenness = nx.betweenness_centrality(T)  \n",
    "    eccentricity = nx.eccentricity(T)  \n",
    "    leaf_node = [v for v, d in T.degree() if d == 1]  \n",
    "    Farness = {v: 1 / (eccentricity[v] + 1) for v in T.nodes() if eccentricity[v] > 0}\n",
    "    \n",
    "    # Calculate subtree heights\n",
    "    subtree_height = {}\n",
    "    for node in T.nodes():\n",
    "        subtree_height[node] = nx.single_source_shortest_path_length(T, node)\n",
    "    for node, distances in subtree_height.items():\n",
    "        subtree_height[node] = max(distances.values()) if distances else 0\n",
    "\n",
    "    features = {}\n",
    "    for v in T:\n",
    "        features[v] = (\n",
    "            degree[v],\n",
    "            closeness[v],\n",
    "            betweenness[v], \n",
    "            eccentricity[v],\n",
    "            leaf_node.count(v),\n",
    "            Farness[v],\n",
    "            subtree_height[v]  # Added subtree height feature\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204be2d",
   "metadata": {},
   "source": [
    "## Split the training data into train and validation sets\n",
    "This cell defines and uses the `split_data_set` function to split the training data into training and validation sets. The split is performed by randomly selecting a subset of unique sentences for validation, ensuring no sentence appears in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a33648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data_set(data, seed=SEED, test_ratio=0.2): # Use global SEED\n",
    "    random.seed(seed)\n",
    "    unique_ids = list(set(data['sentence']))\n",
    "    \n",
    "    # Ensure test_size is not larger than the number of unique_ids\n",
    "    if not unique_ids: # Handle empty data\n",
    "        return pd.DataFrame(columns=data.columns), pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    actual_test_size = int(len(unique_ids) * test_ratio)\n",
    "    if actual_test_size == 0 and len(unique_ids) > 0 and test_ratio > 0: # Ensure at least one sample if possible\n",
    "        actual_test_size = 1 \n",
    "    if actual_test_size > len(unique_ids): # Cap test size\n",
    "        actual_test_size = len(unique_ids)\n",
    "\n",
    "    if actual_test_size == 0 : # if still zero (e.g. unique_ids is empty or test_ratio is 0)\n",
    "         test_ids = set()\n",
    "    else:\n",
    "        test_ids = set(random.sample(unique_ids, actual_test_size))\n",
    "\n",
    "    train_ids = set(unique_ids) - test_ids\n",
    "\n",
    "    train_set = data[data['sentence'].isin(train_ids)]\n",
    "    val_set = data[data['sentence'].isin(test_ids)]\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc5dac",
   "metadata": {},
   "source": [
    "## Utility functions for feature extraction, normalization, and cross-validation\n",
    "\n",
    "Defines utility functions for centrality calculation, data splitting, feature expansion, normalization, and k-fold cross-validation index creation. Also includes the main data preparation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9a5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def centralities(edgelist):\n",
    "    \"\"\"Calculate centrality measures for graph nodes.\"\"\"\n",
    "    if not edgelist:\n",
    "        return {}\n",
    "    \n",
    "    T = nx.from_edgelist(edgelist)\n",
    "    \n",
    "    degree = nx.degree_centrality(T)\n",
    "    closeness = nx.closeness_centrality(T)\n",
    "    betweenness = nx.betweenness_centrality(T)\n",
    "    eccentricity = nx.eccentricity(T)\n",
    "    leaf_node = [v for v, d in T.degree() if d == 1]\n",
    "    farness = {v: 1 / (eccentricity[v] + 1) for v in T.nodes() if eccentricity[v] > 0}\n",
    "    \n",
    "    # Calculate subtree heights\n",
    "    subtree_height = {}\n",
    "    for node in T.nodes():\n",
    "        distances = nx.single_source_shortest_path_length(T, node)\n",
    "        subtree_height[node] = max(distances.values()) if distances else 0\n",
    "\n",
    "    features = {}\n",
    "    for v in T:\n",
    "        features[v] = (\n",
    "            degree[v], closeness[v], betweenness[v], eccentricity[v],\n",
    "            leaf_node.count(v), farness[v], subtree_height[v]\n",
    "        )\n",
    "    return features\n",
    "\n",
    "def split_data_set(data, seed=SEED, test_ratio=0.2):\n",
    "    \"\"\"Split data by unique sentences.\"\"\"\n",
    "    random.seed(seed)\n",
    "    unique_ids = list(set(data['sentence']))\n",
    "    \n",
    "    if not unique_ids:\n",
    "        return pd.DataFrame(columns=data.columns), pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    test_size = max(1, int(len(unique_ids) * test_ratio))\n",
    "    test_size = min(test_size, len(unique_ids))\n",
    "    \n",
    "    if test_size == 0:\n",
    "        test_ids = set()\n",
    "    else:\n",
    "        test_ids = set(random.sample(unique_ids, test_size))\n",
    "    \n",
    "    train_ids = set(unique_ids) - test_ids\n",
    "    \n",
    "    train_set = data[data['sentence'].isin(train_ids)]\n",
    "    val_set = data[data['sentence'].isin(test_ids)]\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def get_expanded_data(data, train=True):\n",
    "    \"\"\"Expand data with centrality features.\"\"\"\n",
    "    expanded_set = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        edgelist_str = row.get('rand_edgelist', '[]')\n",
    "        try:\n",
    "            edgelist = ast.literal_eval(edgelist_str)\n",
    "            if not isinstance(edgelist, list):\n",
    "                edgelist = []\n",
    "        except (ValueError, SyntaxError):\n",
    "            edgelist = []\n",
    "\n",
    "        central_edges = centralities(edgelist)\n",
    "        language = row['language']\n",
    "        sentence = row['sentence']\n",
    "        n = row['n']\n",
    "        \n",
    "        if train:\n",
    "            root = row['root']\n",
    "            for vertex, values in central_edges.items():\n",
    "                expanded_set.append((language, sentence, n, vertex, *values, vertex==root))\n",
    "        else:\n",
    "            row_id = row.get('id', None)\n",
    "            for vertex, values in central_edges.items():\n",
    "                expanded_set.append((row_id, language, sentence, n, vertex, *values))\n",
    "\n",
    "    if train:\n",
    "        columns = ['language', 'sentence', 'n', 'vertex'] + FEATURE_COLUMNS + ['is_root']\n",
    "    else:\n",
    "        columns = ['id', 'language', 'sentence', 'n', 'vertex'] + FEATURE_COLUMNS\n",
    "    \n",
    "    return pd.DataFrame(expanded_set, columns=columns)\n",
    "\n",
    "def normalize_by_sentence(df, feature_columns, groupby_cols=['language', 'sentence']):\n",
    "    \"\"\"Normalize features within each sentence group.\"\"\"\n",
    "    grouped = df.groupby(groupby_cols)\n",
    "    normalized_groups = []\n",
    "    numerical_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    for (lang, sentence), group in grouped:\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized = scaler.fit_transform(group[numerical_features])\n",
    "        normalized_df = pd.DataFrame(normalized, index=group.index, columns=numerical_features)\n",
    "        combined = group.drop(columns=numerical_features).join(normalized_df)\n",
    "        normalized_groups.append(combined)\n",
    "\n",
    "    return pd.concat(normalized_groups)\n",
    "\n",
    "def create_k_folds_indices(data, k=5, seed_base=SEED):\n",
    "    \"\"\"Create k-fold indices for cross-validation.\"\"\"\n",
    "    splits = []\n",
    "    \n",
    "    if not data.index.is_unique:\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "    for i in range(k):\n",
    "        current_seed = seed_base + i\n",
    "        fold_train_set, fold_val_set = split_data_set(data, seed=current_seed, test_ratio=0.2)\n",
    "        \n",
    "        train_indices = data.index.get_indexer(fold_train_set.index.values)\n",
    "        val_indices = data.index.get_indexer(fold_val_set.index.values)\n",
    "        \n",
    "        splits.append((train_indices, val_indices))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare training and test data.\"\"\"\n",
    "    train = pd.read_csv('train-random.csv')\n",
    "    test = pd.read_csv('test-random.csv')\n",
    "    \n",
    "    # Split training data\n",
    "    train_set, val_set = split_data_set(train, seed=SEED, test_ratio=0.2)\n",
    "    \n",
    "    # Expand data with features\n",
    "    expanded_data_train = get_expanded_data(train_set)\n",
    "    expanded_data_val = get_expanded_data(val_set)\n",
    "    expanded_data_test = get_expanded_data(test, train=False)\n",
    "    \n",
    "    # Sort data\n",
    "    expanded_data_train.sort_values(by=['language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "    expanded_data_val.sort_values(by=['language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "    expanded_data_test.sort_values(by=['id', 'language', 'sentence', 'n', 'vertex'], inplace=True)\n",
    "    \n",
    "    # Normalize features\n",
    "    train_scaled = normalize_by_sentence(expanded_data_train, FEATURE_COLUMNS)\n",
    "    val_scaled = normalize_by_sentence(expanded_data_val, FEATURE_COLUMNS)\n",
    "    test_scaled = normalize_by_sentence(expanded_data_test, FEATURE_COLUMNS)\n",
    "    \n",
    "    return train_scaled, val_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aff22c",
   "metadata": {},
   "source": [
    "## Prepare the data for training and validation\n",
    "\n",
    "Calls the `prepare_data` function to load, process, and normalize the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57c409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, val_scaled, test_scaled = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f5d3d",
   "metadata": {},
   "source": [
    "## Resample the training data and set up hyperparameter search\n",
    "\n",
    "Resamples the training data using SMOTEENN to address class imbalance, sets up the RandomForestClassifier, defines the hyperparameter search space, and configures the RandomizedSearchCV for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3484cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "train_resampled = train_scaled.drop(columns=['language', 'n', 'vertex'])\n",
    "\n",
    "smoteenn = SMOTEENN(random_state=42, sampling_strategy=0.5, n_jobs=-1)\n",
    "\n",
    "train_resampled, _ = smoteenn.fit_resample(train_resampled, train_resampled.is_root)\n",
    "\n",
    "splits = create_k_folds_indices(train_resampled)\n",
    "\n",
    "X_resampled = train_resampled.drop(columns=['sentence', 'is_root'])\n",
    "y_resampled = train_resampled['is_root'].astype(int)\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=SEED, n_jobs=-1,oob_score=True)\n",
    "\n",
    "ntrees = [100, 200, 300]\n",
    "max_depth = [50, 100, 200]\n",
    "min_samples_split = [4, 6, 8]\n",
    "min_samples_leaf = [2, 4, 6]\n",
    "balance = ['balanced', 'balanced_subsample']\n",
    "max_features = ['sqrt', 'log2', None, 0.5, 0.7]\n",
    "criterion = ['gini', 'entropy']\n",
    "bootstrap = [True, False]\n",
    "\n",
    "f1_class_0_scorer = make_scorer(f1_score, pos_label=0)\n",
    "f1_class_1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "scoring_dict = {\n",
    "    'f1_mac': 'f1_macro',\n",
    "    'f1_class_0': f1_class_0_scorer,\n",
    "    'f1_class_1': f1_class_1_scorer,\n",
    "    'acc': 'accuracy'\n",
    "}\n",
    "\n",
    "trc = RandomizedSearchCV(estimator=rf_model,\n",
    "                   scoring=scoring_dict,\n",
    "                   param_distributions={\n",
    "                       'n_estimators': ntrees,\n",
    "                       'max_depth':max_depth,\n",
    "                       'min_samples_split':min_samples_split,\n",
    "                       'min_samples_leaf':min_samples_leaf, \n",
    "                       'class_weight':balance,\n",
    "                       'max_features': max_features,\n",
    "                       'criterion': criterion,\n",
    "                       'bootstrap': bootstrap\n",
    "                   },\n",
    "                   n_iter=500,\n",
    "                   cv=splits,\n",
    "                   return_train_score=False,\n",
    "                   refit='f1_mac',\n",
    "                   random_state=SEED,\n",
    "                   n_jobs=-1)\n",
    "\n",
    "model_5CV = trc.fit(X_resampled, y_resampled)\n",
    "importances = model_5CV.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b713eda",
   "metadata": {},
   "source": [
    "## Analyze and save cross-validation results\n",
    "\n",
    "Creates a DataFrame from the cross-validation results, saves them to a CSV file, and displays the top results sorted by mean macro F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3979c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>mean_test_f1_mac</th>\n",
       "      <th>mean_test_f1_class_0</th>\n",
       "      <th>mean_test_f1_class_1</th>\n",
       "      <th>mean_test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886425</td>\n",
       "      <td>0.926852</td>\n",
       "      <td>0.845997</td>\n",
       "      <td>0.900831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886425</td>\n",
       "      <td>0.926852</td>\n",
       "      <td>0.845997</td>\n",
       "      <td>0.900831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886040</td>\n",
       "      <td>0.926601</td>\n",
       "      <td>0.845480</td>\n",
       "      <td>0.900492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886040</td>\n",
       "      <td>0.926601</td>\n",
       "      <td>0.845480</td>\n",
       "      <td>0.900492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>0.926560</td>\n",
       "      <td>0.845409</td>\n",
       "      <td>0.900440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     param_n_estimators  param_max_depth  param_min_samples_split  \\\n",
       "365                 200              100                        6   \n",
       "296                 200              200                        6   \n",
       "452                 300              100                        6   \n",
       "376                 300              200                        6   \n",
       "420                 300               50                        6   \n",
       "\n",
       "     param_min_samples_leaf param_class_weight param_max_features  \\\n",
       "365                       2           balanced               sqrt   \n",
       "296                       2           balanced               sqrt   \n",
       "452                       2           balanced               log2   \n",
       "376                       2           balanced               sqrt   \n",
       "420                       2           balanced               sqrt   \n",
       "\n",
       "    param_criterion  param_bootstrap  mean_test_f1_mac  mean_test_f1_class_0  \\\n",
       "365         entropy             True          0.886425              0.926852   \n",
       "296         entropy             True          0.886425              0.926852   \n",
       "452         entropy             True          0.886040              0.926601   \n",
       "376         entropy             True          0.886040              0.926601   \n",
       "420         entropy             True          0.885985              0.926560   \n",
       "\n",
       "     mean_test_f1_class_1  mean_test_acc  \n",
       "365              0.845997       0.900831  \n",
       "296              0.845997       0.900831  \n",
       "452              0.845480       0.900492  \n",
       "376              0.845480       0.900492  \n",
       "420              0.845409       0.900440  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_cols = [\n",
    "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
    "    'param_min_samples_leaf', 'param_class_weight', 'param_max_features',\n",
    "    'param_criterion', 'param_bootstrap',\n",
    "    'mean_test_f1_mac', 'mean_test_f1_class_0', 'mean_test_f1_class_1',\n",
    "    'mean_test_acc'\n",
    "]\n",
    "# Create DataFrame from CV results\n",
    "cv_results_df = pd.DataFrame(model_5CV.cv_results_)\n",
    "\n",
    "# Save all results to CSV\n",
    "cv_results_df.to_csv('randomized_search_results.csv', index=False)\n",
    "\n",
    "pd.DataFrame(model_5CV.cv_results_).sort_values(by='mean_test_f1_mac', ascending=False)[scoring_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc721eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>mean_test_f1_mac</th>\n",
       "      <th>mean_test_f1_class_0</th>\n",
       "      <th>mean_test_f1_class_1</th>\n",
       "      <th>mean_test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886425</td>\n",
       "      <td>0.926852</td>\n",
       "      <td>0.845997</td>\n",
       "      <td>0.900831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886425</td>\n",
       "      <td>0.926852</td>\n",
       "      <td>0.845997</td>\n",
       "      <td>0.900831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886040</td>\n",
       "      <td>0.926601</td>\n",
       "      <td>0.845480</td>\n",
       "      <td>0.900492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.886040</td>\n",
       "      <td>0.926601</td>\n",
       "      <td>0.845480</td>\n",
       "      <td>0.900492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885985</td>\n",
       "      <td>0.926560</td>\n",
       "      <td>0.845409</td>\n",
       "      <td>0.900440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885785</td>\n",
       "      <td>0.926426</td>\n",
       "      <td>0.845144</td>\n",
       "      <td>0.900261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885785</td>\n",
       "      <td>0.926426</td>\n",
       "      <td>0.845144</td>\n",
       "      <td>0.900261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885722</td>\n",
       "      <td>0.926359</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.900188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885712</td>\n",
       "      <td>0.926222</td>\n",
       "      <td>0.845202</td>\n",
       "      <td>0.900087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>0.885711</td>\n",
       "      <td>0.926505</td>\n",
       "      <td>0.844916</td>\n",
       "      <td>0.900286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     param_n_estimators  param_max_depth  param_min_samples_split  \\\n",
       "365                 200              100                        6   \n",
       "296                 200              200                        6   \n",
       "452                 300              100                        6   \n",
       "376                 300              200                        6   \n",
       "420                 300               50                        6   \n",
       "140                 100              200                        6   \n",
       "436                 100              200                        6   \n",
       "412                 100               50                        6   \n",
       "362                 200               50                        8   \n",
       "308                 300               50                        4   \n",
       "\n",
       "     param_min_samples_leaf  param_class_weight param_max_features  \\\n",
       "365                       2            balanced               sqrt   \n",
       "296                       2            balanced               sqrt   \n",
       "452                       2            balanced               log2   \n",
       "376                       2            balanced               sqrt   \n",
       "420                       2            balanced               sqrt   \n",
       "140                       2            balanced               log2   \n",
       "436                       2            balanced               sqrt   \n",
       "412                       2            balanced               sqrt   \n",
       "362                       2            balanced               log2   \n",
       "308                       2  balanced_subsample               log2   \n",
       "\n",
       "    param_criterion  param_bootstrap  mean_test_f1_mac  mean_test_f1_class_0  \\\n",
       "365         entropy             True          0.886425              0.926852   \n",
       "296         entropy             True          0.886425              0.926852   \n",
       "452         entropy             True          0.886040              0.926601   \n",
       "376         entropy             True          0.886040              0.926601   \n",
       "420         entropy             True          0.885985              0.926560   \n",
       "140         entropy             True          0.885785              0.926426   \n",
       "436         entropy             True          0.885785              0.926426   \n",
       "412         entropy             True          0.885722              0.926359   \n",
       "362         entropy             True          0.885712              0.926222   \n",
       "308         entropy             True          0.885711              0.926505   \n",
       "\n",
       "     mean_test_f1_class_1  mean_test_acc  \n",
       "365              0.845997       0.900831  \n",
       "296              0.845997       0.900831  \n",
       "452              0.845480       0.900492  \n",
       "376              0.845480       0.900492  \n",
       "420              0.845409       0.900440  \n",
       "140              0.845144       0.900261  \n",
       "436              0.845144       0.900261  \n",
       "412              0.845085       0.900188  \n",
       "362              0.845202       0.900087  \n",
       "308              0.844916       0.900286  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show only selected columns for the top 10 results sorted by mean_test_f1_mac\n",
    "\n",
    "selected_cols = [\n",
    "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
    "    'param_min_samples_leaf', 'param_class_weight', 'param_max_features',\n",
    "    'param_criterion', 'param_bootstrap',\n",
    "    'mean_test_f1_mac', 'mean_test_f1_class_0', 'mean_test_f1_class_1',\n",
    "    'mean_test_acc'\n",
    "]\n",
    "\n",
    "top10 = cv_results_df.sort_values(by='mean_test_f1_mac', ascending=False).head(10)\n",
    "top10[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a29a7d",
   "metadata": {},
   "source": [
    "## Extract best hyperparameters from cross-validation\n",
    "\n",
    "Finds and displays the best hyperparameter set based on the highest mean macro F1 score from the cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd738a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 6,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 100,\n",
       " 'criterion': 'entropy',\n",
       " 'class_weight': 'balanced',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = pd.DataFrame(model_5CV.cv_results_).sort_values(by='mean_test_f1_mac',ascending=False)[['params']].iloc[0,0]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f43330",
   "metadata": {},
   "source": [
    "## Train the final model with best hyperparameters and evaluate\n",
    "\n",
    "Trains a new RandomForestClassifier using the best hyperparameters, fits it to the resampled training data, saves the model, and evaluates its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686340ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95     38134\n",
      "           1       0.23      0.34      0.27      2100\n",
      "\n",
      "    accuracy                           0.90     40234\n",
      "   macro avg       0.59      0.64      0.61     40234\n",
      "weighted avg       0.92      0.90      0.91     40234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_tuned = RandomForestClassifier(**best_params)\n",
    "rf_model_tuned.fit(X_resampled, y_resampled)\n",
    "\n",
    "#save the model\n",
    "joblib.dump(rf_model_tuned, './rf_model_tuned.pkl')\n",
    "\n",
    "X_val = val_scaled.drop(columns=['language', 'n', 'vertex', 'sentence', 'is_root'])\n",
    "y_val = val_scaled['is_root'].astype(int)\n",
    "\n",
    "y_pred = rf_model_tuned.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971eb7d6",
   "metadata": {},
   "source": [
    "## Generate predictions for the test set and create submission file\n",
    "\n",
    "Uses the tuned model to predict probabilities for the test set, selects the most likely root for each graph, and saves the results in a submission CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9919158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "X_test = test_scaled.drop(columns=['id', 'language', 'n', 'vertex', 'sentence'])\n",
    "\n",
    "y_pred_proba = rf_model_tuned.predict_proba(X_test)[:, 1]  # Get probability of positive class\n",
    "\n",
    "test_scaled['probability'] = y_pred_proba\n",
    "submission = {}\n",
    "# Group predictions by ID\n",
    "grouped = test_scaled.groupby('id')\n",
    "\n",
    "for graph_id, group in grouped:\n",
    "    # Get the row with the highest predicted root probability\n",
    "    best_row = group.loc[group['probability'].idxmax()]\n",
    "    submission[graph_id] = best_row['vertex']\n",
    "\n",
    "sub = pd.DataFrame(submission.items(), columns=['id','root'])\n",
    "\n",
    "sub['root'] = sub['root'].astype(int)\n",
    "print(type(sub.root.iloc[0]))\n",
    "\n",
    "sub.to_csv(\"./submission.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
